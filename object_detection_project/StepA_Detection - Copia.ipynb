{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a0fb41",
   "metadata": {},
   "source": [
    "# Step A: Multi-Product Detection on Store Shelves\n",
    "\n",
    "In this notebook, we will implement an object detection pipeline using ORB features and homography  \n",
    "to recognize multiple cereal boxes in shelf images.\n",
    "\n",
    "**Outline:**\n",
    "1. Import dependencies  \n",
    "2. Configure paths and ORB parameters  \n",
    "3. Extract features and match  \n",
    "4. Filter matches and estimate homography  \n",
    "5. Compute bounding boxes (center, width, height)  \n",
    "6. Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a775d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import dependencies\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# Enable inline plotting\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24e9589",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "- `REF_DIR`: Folder containing reference images (`models/`)  \n",
    "- `SCENE_DIR`: Folder containing scene images (`scenes/`)  \n",
    "- `MAX_FEATURES`: Maximum number of ORB keypoints to detect  \n",
    "- `GOOD_MATCH_RATIO`: Lowe’s ratio threshold for good matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d4310b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Set paths and SIFT parameters\n",
    "REF_DIR = 'models'\n",
    "SCENE_DIR = 'scenes'\n",
    "REF_FILES = [\n",
    "    \"0.jpg\", \"1.jpg\", \"11.jpg\", \"19.jpg\",\n",
    "    \"24.jpg\", \"25.jpg\", \"26.jpg\"\n",
    "]\n",
    "\n",
    "MAX_FEATURES    = 0      # unused by SIFT\n",
    "GOOD_MATCH_RATIO = 0.75  # Lowe’s ratio\n",
    "MIN_MATCHES      = 8\n",
    "MIN_INLIERS      = 15\n",
    "\n",
    "# Initialize SIFT detector + L2 brute‑force matcher\n",
    "sift = cv2.SIFT_create()                  \n",
    "bf   = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edd9c97",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction and Matching\n",
    "\n",
    "Define two helper functions:\n",
    "- `detect_and_compute(img)`: detect ORB keypoints & compute descriptors  \n",
    "- `match_features(des1, des2, ratio)`: match descriptors via k‑NN and apply Lowe’s ratio test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8188128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define feature extraction and matching functions with SIFT\n",
    "\n",
    "def detect_and_compute(img):\n",
    "    \"\"\"\n",
    "    Detect SIFT keypoints and compute descriptors.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    kp, des = sift.detectAndCompute(gray, None)\n",
    "    return kp, des\n",
    "\n",
    "def match_features(des1, des2, ratio=GOOD_MATCH_RATIO):\n",
    "    \"\"\"\n",
    "    Match descriptors using k-NN and apply Lowe's ratio test.\n",
    "    Returns a list of good matches.\n",
    "    \"\"\"\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < ratio * n.distance:\n",
    "            good.append(m)\n",
    "    return good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f26ede8",
   "metadata": {},
   "source": [
    "## 4. Homography Estimation & Bounding Box Calculation\n",
    "\n",
    "Function `detect_instances` will:\n",
    "1. Extract & match features between reference and scene  \n",
    "2. Check for at least `min_matches` good matches  \n",
    "3. Estimate homography via RANSAC  \n",
    "4. Project reference corners into the scene  \n",
    "5. Compute bounding‑box center, width, height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92394d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define instance detection function with debug\n",
    "def detect_instances(ref_img, scene_img, min_inliers=MIN_INLIERS):\n",
    "    \"\"\"\n",
    "    Try to find exactly one instance of ref_img in scene_img.\n",
    "    Returns (center, (w,h), corners, inliers) if inliers>=min_inliers, else None.\n",
    "    \"\"\"\n",
    "    # extract features\n",
    "    ref_kp, ref_des     = detect_and_compute(ref_img)\n",
    "    scene_kp, scene_des = detect_and_compute(scene_img)\n",
    "\n",
    "    # match + ratio test\n",
    "    matches = bf.knnMatch(ref_des, scene_des, k=2)\n",
    "    good    = [m for m,n in matches if m.distance < GOOD_MATCH_RATIO * n.distance]\n",
    "    if len(good) < 8:         # quick pre‐filter  \n",
    "        return None\n",
    "\n",
    "    # homography\n",
    "    src_pts = np.float32([ref_kp[m.queryIdx].pt for m in good]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([scene_kp[m.trainIdx].pt for m in good]).reshape(-1,1,2)\n",
    "    H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "    if H is None:\n",
    "        return None\n",
    "\n",
    "    inliers = int(mask.sum())\n",
    "    if inliers < min_inliers:\n",
    "        return None\n",
    "\n",
    "    # project corners\n",
    "    h_ref, w_ref = ref_img.shape[:2]\n",
    "    corners0 = np.float32([[0,0],[w_ref,0],[w_ref,h_ref],[0,h_ref]]).reshape(-1,1,2)\n",
    "    proj_c   = cv2.perspectiveTransform(corners0, H).reshape(-1,2)\n",
    "    x_min, y_min = proj_c.min(axis=0)\n",
    "    x_max, y_max = proj_c.max(axis=0)\n",
    "    w_box, h_box = x_max-x_min, y_max-y_min\n",
    "    center = (x_min + w_box/2, y_min + h_box/2)\n",
    "\n",
    "    return center, (w_box, h_box), proj_c, inliers\n",
    "\n",
    "    \"\"\"\n",
    "    Detect instances of ref_img within scene_img.\n",
    "    Prints number of good matches for debugging.\n",
    "    Returns a list of tuples: (center, (width, height), projected_corners).\n",
    "    \"\"\"\n",
    "    # 1) Extract features\n",
    "    ref_kp, ref_des = detect_and_compute(ref_img)\n",
    "    scene_kp, scene_des = detect_and_compute(scene_img)\n",
    "    \n",
    "    # 2) Match and filter\n",
    "    good = match_features(ref_des, scene_des)\n",
    "    print(f\"    Found {len(good)} good matches\")  # debug\n",
    "    \n",
    "    # 3) Check threshold\n",
    "    if len(good) < min_matches:\n",
    "        return []\n",
    "    \n",
    "    # 4) Prepare points for homography\n",
    "    src_pts = np.float32([ref_kp[m.queryIdx].pt for m in good]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([scene_kp[m.trainIdx].pt for m in good]).reshape(-1,1,2)\n",
    "    \n",
    "    # 5) Estimate homography\n",
    "    H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "    if H is None:\n",
    "        return []\n",
    "    \n",
    "    # 6) Project corners and compute bbox\n",
    "    h_ref, w_ref = ref_img.shape[:2]\n",
    "    corners = np.float32([[0,0], [w_ref,0], [w_ref,h_ref], [0,h_ref]]).reshape(-1,1,2)\n",
    "    proj_corners = cv2.perspectiveTransform(corners, H).reshape(-1,2)\n",
    "    x_min, y_min = proj_corners.min(axis=0)\n",
    "    x_max, y_max = proj_corners.max(axis=0)\n",
    "    w_box, h_box = x_max - x_min, y_max - y_min\n",
    "    center = (x_min + w_box/2, y_min + h_box/2)\n",
    "    \n",
    "    return [(center, (w_box, h_box), proj_corners)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b0dd188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suppress_nested(instances, iou_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Rimuove le istanze i cui bounding-box sono in gran parte contenuti in altri.\n",
    "    - instances: lista di tuple (center, (w,h), corners, inliers)\n",
    "    - iou_thresh: soglia di contenimento (fra 0 e 1)\n",
    "    Restituisce la lista filtered senza i box troppo nidificati.\n",
    "    \"\"\"\n",
    "    # Costruisco una lista di box axis-aligned [x1,y1,x2,y2,inliers,idx]\n",
    "    boxes = []\n",
    "    for idx, (center, (w, h), corners, inliers) in enumerate(instances):\n",
    "        x1, y1 = corners.min(axis=0)\n",
    "        x2, y2 = corners.max(axis=0)\n",
    "        boxes.append([x1, y1, x2, y2, inliers, idx])\n",
    "    # Ordino per inlier decrescente (più robusto primo)\n",
    "    boxes.sort(key=lambda b: b[4], reverse=True)\n",
    "\n",
    "    keep = []\n",
    "    for b in boxes:\n",
    "        x1,y1,x2,y2,ins,i = b\n",
    "        discard = False\n",
    "        for kb in keep:\n",
    "            xx1,yy1,xx2,yy2,_,_ = kb\n",
    "            # calcolo area d’intersezione\n",
    "            ix1, iy1 = max(x1,xx1), max(y1,yy1)\n",
    "            ix2, iy2 = min(x2,xx2), min(y2,yy2)\n",
    "            if ix2 > ix1 and iy2 > iy1:\n",
    "                inter = (ix2-ix1)*(iy2-iy1)\n",
    "                area_b = (x2-x1)*(y2-y1)\n",
    "                if inter/area_b > iou_thresh:\n",
    "                    discard = True\n",
    "                    break\n",
    "        if not discard:\n",
    "            keep.append(b)\n",
    "\n",
    "    # ricostruisco la lista delle istanze tenute\n",
    "    filtered = [instances[b[5]] for b in keep]\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36abd05",
   "metadata": {},
   "source": [
    "## 5. Execution & Visualization\n",
    "\n",
    "Loop over all scenes, detect each reference, draw bounding boxes, and display both  \n",
    "the annotated image and detection details in the console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# —— Cell: definizione delle funzioni di rilevamento multiplo e soppressione ——\n",
    "\n",
    "def detect_multiple_instances(ref_img, scene_img,\n",
    "                              min_inliers=MIN_INLIERS,\n",
    "                              ratio=GOOD_MATCH_RATIO,\n",
    "                              max_iter=5):\n",
    "    \"\"\"\n",
    "    Cerca fino a max_iter istanze di ref_img in scene_img:\n",
    "    - estrae feature SIFT\n",
    "    - knnMatch + Lowe’s ratio\n",
    "    - itera RANSAC, rimuovendo gli inlier trovati\n",
    "    Restituisce lista di (center, (w,h), corners, inliers).\n",
    "    \"\"\"\n",
    "    ref_kp, ref_des     = detect_and_compute(ref_img)\n",
    "    scene_kp, scene_des = detect_and_compute(scene_img)\n",
    "    matches = bf.knnMatch(ref_des, scene_des, k=2)\n",
    "    good    = [m for m,n in matches if m.distance < ratio * n.distance]\n",
    "\n",
    "    instances = []\n",
    "    for _ in range(max_iter):\n",
    "        if len(good) < 8:\n",
    "            break\n",
    "\n",
    "        src_pts = np.float32([ref_kp[m.queryIdx].pt for m in good]).reshape(-1,1,2)\n",
    "        dst_pts = np.float32([scene_kp[m.trainIdx].pt for m in good]).reshape(-1,1,2)\n",
    "        H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "        if H is None:\n",
    "            break\n",
    "\n",
    "        inlier_mask = mask.ravel().astype(bool)\n",
    "        inliers     = int(inlier_mask.sum())\n",
    "        if inliers < min_inliers:\n",
    "            break\n",
    "\n",
    "        # rifinitura omografia su inlier\n",
    "        src_in = src_pts[inlier_mask]\n",
    "        dst_in = dst_pts[inlier_mask]\n",
    "        H_ref, _ = cv2.findHomography(src_in, dst_in, 0)\n",
    "\n",
    "        # proiezione degli angoli reali\n",
    "        h_ref, w_ref = ref_img.shape[:2]\n",
    "        corners0 = np.float32([[0,0],[w_ref,0],[w_ref,h_ref],[0,h_ref]]).reshape(-1,1,2)\n",
    "        corners_proj = cv2.perspectiveTransform(corners0, H_ref).reshape(-1,2)\n",
    "\n",
    "        # centro e dimensioni axis-aligned\n",
    "        x_min, y_min = corners_proj.min(axis=0)\n",
    "        x_max, y_max = corners_proj.max(axis=0)\n",
    "        center = ((x_min + x_max)/2, (y_min + y_max)/2)\n",
    "        dims   = (x_max - x_min, y_max - y_min)\n",
    "\n",
    "        instances.append((center, dims, corners_proj.astype(int), inliers))\n",
    "\n",
    "        # rimuovo gli inlier per la prossima iterazione\n",
    "        good = [m for idx, m in enumerate(good) if not inlier_mask[idx]]\n",
    "\n",
    "    return instances\n",
    "\n",
    "\n",
    "def suppress_nested(instances, iou_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Rimuove le istanze i cui bounding-box (axis-aligned) sono per >iou_thresh\n",
    "    contenute in un'altra detection più robusta.\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    for idx, (center, (w,h), corners, inliers) in enumerate(instances):\n",
    "        x1, y1 = corners.min(axis=0)\n",
    "        x2, y2 = corners.max(axis=0)\n",
    "        boxes.append([x1, y1, x2, y2, inliers, idx])\n",
    "\n",
    "    boxes.sort(key=lambda b: b[4], reverse=True)\n",
    "    keep = []\n",
    "    for b in boxes:\n",
    "        x1,y1,x2,y2,ins,i = b\n",
    "        discard = False\n",
    "        for kb in keep:\n",
    "            xx1,yy1,xx2,yy2,_,_ = kb\n",
    "            ix1, iy1 = max(x1,xx1), max(y1,yy1)\n",
    "            ix2, iy2 = min(x2,xx2), min(y2,yy2)\n",
    "            if ix2>ix1 and iy2>iy1:\n",
    "                inter = (ix2-ix1)*(iy2-iy1)\n",
    "                area = (x2-x1)*(y2-y1)\n",
    "                if inter/area > iou_thresh:\n",
    "                    discard = True\n",
    "                    break\n",
    "        if not discard:\n",
    "            keep.append(b)\n",
    "\n",
    "    return [instances[b[5]] for b in keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edd8bae6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detect_multiple_instances' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ref_file \u001b[38;5;129;01min\u001b[39;00m REF_FILES:\n\u001b[32m     10\u001b[39m     ref_img = cv2.imread(os.path.join(REF_DIR, ref_file))\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     insts   = \u001b[43mdetect_multiple_instances\u001b[49m(ref_img, scene)\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# ➊ elimino le istanze troppo nidificate\u001b[39;00m\n\u001b[32m     14\u001b[39m     insts = suppress_nested(insts, iou_thresh=\u001b[32m0.5\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'detect_multiple_instances' is not defined"
     ]
    }
   ],
   "source": [
    "# 5. Run multi-instance detection with suppression of nested boxes\n",
    "for scene_file in sorted(os.listdir(SCENE_DIR)):\n",
    "    if not scene_file.lower().endswith(('.png','.jpg','.jpeg')):\n",
    "        continue\n",
    "\n",
    "    scene = cv2.imread(os.path.join(SCENE_DIR, scene_file))\n",
    "    disp  = scene.copy()\n",
    "\n",
    "    for ref_file in REF_FILES:\n",
    "        ref_img = cv2.imread(os.path.join(REF_DIR, ref_file))\n",
    "        insts   = detect_multiple_instances(ref_img, scene)\n",
    "\n",
    "        # ➊ elimino le istanze troppo nidificate\n",
    "        insts = suppress_nested(insts, iou_thresh=0.5)\n",
    "\n",
    "        # ➋ disegno solo quelle rimaste\n",
    "        for center, (w_box, h_box), corners, inliers in insts:\n",
    "            pts = corners.reshape(-1,1,2)\n",
    "            cv2.polylines(disp, [pts], True, (0,255,0), 2)\n",
    "            x0, y0 = corners[0]\n",
    "            cv2.putText(disp, ref_file, (int(x0), int(y0)-5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "            print(f\"{scene_file} – {ref_file}: inliers={inliers}, \"\n",
    "                  f\"center=({center[0]:.1f},{center[1]:.1f}), \"\n",
    "                  f\"w={w_box:.1f}, h={h_box:.1f}\")\n",
    "\n",
    "    # Mostro l’immagine annotata\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.title(f\"Detections in {scene_file}\")\n",
    "    plt.axis('off')\n",
    "    plt.imshow(cv2.cvtColor(disp, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
